{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.4 Support Vector Machines\n",
    "\n",
    "Support vector machines (SVMs) are a set of supervised learning methods used for \n",
    "- classiﬁcation\n",
    "- regression\n",
    "- outliers detection\n",
    "\n",
    "Advantages:\n",
    "- Effective in high dimensional spaces\n",
    "- Still effective in cases where number of dimensions is greater than the number of samples\n",
    "- Uses a subset of training points in the decision function (called support vectors), so it is also memory efﬁcient\n",
    "- Versatile: different Kernel functions can be speciﬁed for the decision function. Common kernels are provided, but it is also possible to specify custom kernels\n",
    "\n",
    "Disadvantages:\n",
    "- If the number of features is much greater than the number of samples, avoid over-ﬁtting in choosing Kernel functions and regularization term is crucial.\n",
    "-  SVMs do not directly provide probability estimates, these are calculated using an expensive ﬁve-fold crossvalidation (see Scores and probabilities, below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Classiﬁcation\n",
    "\n",
    "SVC, NuSVC and LinearSVC are classes capable of performing multi-class classiﬁcation on a dataset.\n",
    "\n",
    "SVC and NuSVC are similar methods, but accept slightly different sets of parameters and have different mathematical formulations (see section Mathematical formulation). On the other hand, LinearSVC is another implementation of Support Vector Classiﬁcation for the case of a linear kernel. Note that LinearSVC does not accept keyword kernel, as this is assumed to be linear. It also lacks some of the members of SVC and NuSVC, like support_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[[0. 0.]\n",
      " [1. 1.]]\n",
      "[0 1]\n",
      "[1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "X = [[0, 0], [1, 1]] \n",
    "y = [0, 1]\n",
    "clf = svm.SVC(gamma='scale') \n",
    "clf.fit(X, y)\n",
    "print(clf.predict([[2., 2.]]) )\n",
    "# get support vectors \n",
    "print(clf.support_vectors_)\n",
    "# get indices of support vectors \n",
    "print(clf.support_)\n",
    "# get number of support vectors for each class \n",
    "print(clf.n_support_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1) Multi-class classiﬁcation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovo', degree=3, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "(1, 6)\n",
      "(1, 4)\n"
     ]
    }
   ],
   "source": [
    "X = [[0], [1], [2], [3]]\n",
    "Y = [0, 1, 2, 3]\n",
    "clf = svm.SVC(gamma='scale', decision_function_shape='ovo')\n",
    "print(clf.fit(X, Y))\n",
    "dec = clf.decision_function([[1]])\n",
    "print(dec.shape)\n",
    "clf.decision_function_shape = 'ovr'\n",
    "dec = clf.decision_function([[1]])\n",
    "print(dec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, LinearSVC implements “one-vs-the-rest” multi-class strategy, thus training n_class models. If there are only two classes, only one model is trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "          verbose=0)\n",
      "(1, 4)\n"
     ]
    }
   ],
   "source": [
    "lin_clf = svm.LinearSVC() \n",
    "print(lin_clf.fit(X, Y))\n",
    "dec = lin_clf.decision_function([[1]])\n",
    "print(dec.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2) Unbalanced problems\n",
    "\n",
    "In problems where it is desired to give more importance to certain classes or certain individual samples keywords class_weight and sample_weight can be used.\n",
    "\n",
    "SVC (but not NuSVC) implement a keyword class_weight in the fit method. It’s a dictionary of the form {class_label : value}, where value is a ﬂoating point number > 0 that sets the parameter C of class class_label to C * value.\n",
    "\n",
    "SVC, NuSVC, SVR, NuSVR and OneClassSVM implement also weights for individual samples in method fit through keyword sample_weight. Similar to class_weight, these set the parameter C for the i-th example to C * sample_weight \\[i\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Regression\n",
    "\n",
    "The method of Support Vector Classiﬁcation can be extended to solve regression problems. This method is called Support Vector Regression. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
