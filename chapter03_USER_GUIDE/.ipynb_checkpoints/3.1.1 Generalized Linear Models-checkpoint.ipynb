{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.1 Generalized Linear Models\n",
    "\n",
    "The following are a set of methods intended for regression in which the target value is expected to be a linear combination of the features.\n",
    "\n",
    ">$\\hat{y}(w,x) = w_0 + w_1 x_1 + ... + w_p x_p$\n",
    "\n",
    "The vector $w = (w_1,...,w_p)$ as coef_ and the $w_0$ as intercept_\n",
    "\n",
    "To perform classiﬁcation with generalized linear models, see Logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1.1 Ordinary Least Squares\n",
    "\n",
    "**Complexity**: $O(n_{samples}n^2_{features})$\n",
    "\n",
    "*LinearRegression* fits a linear model with coefficients $w = (w_1,...,w_p)$ to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted bu the linear approximation.\n",
    ">$\\min_{w}||X w - y||^2_2$\n",
    "\n",
    "LinearRegression will take in its fit method arrays X, y and will store the coefﬁcients w of the linear model in its coef_ member\n",
    "\n",
    "**Assumption**: the independence of the features. \n",
    "\n",
    "When features are correlated and the columns of the design matrix $X$ have an approximate linear dependence, the design matrix becomes close to singular and as a result, the least-squares estimate becomes highly sensitive to random errors in the observed target, producing a large variance. This situation of multicollinearity can arise, for example, when data are collected without an experimental design.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
      "         normalize=False)\n",
      "[0.5 0.5]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression()\n",
    "print(reg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2]))\n",
    "print(reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1.2 Ridge Regression\n",
    "\n",
    "**Complexity**: $O(n_{samples}n^2_{features})$\n",
    "\n",
    "Ridge regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of the coefﬁcients. \n",
    "> $\\min_{w}{||X w - y||^2_2 + \\alpha||w||^2_2}$\n",
    "\n",
    "The complexity parameter $ \\alpha >= 0$ controls the amount of shrinkage: the larger the value of $\\alpha$, the greater the amount of shrinkage and thus the coefﬁcients become more robust to collinearity.\n",
    "\n",
    "**Assumption**: the multicollinearity between the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "[0.34545455 0.34545455]\n",
      "0.1363636363636364\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.Ridge(alpha=.5)\n",
    "print(reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1]))\n",
    "print(reg.coef_)\n",
    "print(reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RidgeCV implements ridge regression with built-in cross-validation of the alpha parameter. The object works in the same way as GridSearchCV except that it defaults to Generalized Cross-Validation (GCV), an efﬁcient form of leave-one-out cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RidgeCV(alphas=array([1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01,\n",
      "       1.e+02, 1.e+03, 1.e+04, 1.e+05, 1.e+06]),\n",
      "    cv=None, fit_intercept=True, gcv_mode=None, normalize=False,\n",
      "    scoring=None, store_cv_values=False)\n",
      "0.01\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "reg = linear_model.RidgeCV(alphas=np.logspace(-6,6,13))\n",
    "print(reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1]))\n",
    "print(reg.alpha_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1.3 Lasso\n",
    "\n",
    "The Lasso is a linear model that estimates sparse coefﬁcients. It is useful in some contexts due to its tendency to prefer solutions with fewer non-zero coefﬁcients, effectively reducing the number of features upon which the given solution is dependent. For this reason Lasso and its variants are fundamental to the ﬁeld of compressed sensing.\n",
    "\n",
    "Under certain conditions, it can recover the exact set of non-zero coefﬁcients.\n",
    "\n",
    ">$\\min_{w}{\\frac{1}{2n_{samples}}||X w - y||^2_2 + \\alpha||w||_2}$\n",
    "\n",
    "The implementation in the class Lasso uses coordinate descent as the algorithm to ﬁt the coefﬁcients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=None,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "[0.6 0. ]\n",
      "[0.8]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.Lasso(alpha=.1)\n",
    "print(reg.fit([[0, 0], [1, 1]], [0, 1]))\n",
    "print(reg.coef_)\n",
    "print(reg.predict([[1,1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using cross-validation\n",
    "\n",
    "scikit-learn exposes objects that set the Lasso alpha parameter by cross-validation: LassoCV and LassoLarsCV. LassoLarsCV is based on the Least Angle Regression algorithm.\n",
    "\n",
    "- LassoCV: For high-dimentional datasets with many collinear features\n",
    "- LassoLarsCV:\n",
    "    - exploring more relevant values of alpha parameter\n",
    "    - faster if the number of samples is very small compared to the number of features\n",
    "    \n",
    "#### Information-criteria based model selection\n",
    "\n",
    "Alternatively, the estimator LassoLarsIC proposes to use the Akaike information criterion (AIC) and the Bayes Information criterion (BIC). It is a computationally cheaper alternative to ﬁnd the optimal value of alpha as the regularization path is computed only once insteadof k+1 times when using k-fold cross-validation. However, such criteria needs a proper estimation of the degrees of freedom of the solution, are derived for large samples (asymptotic results) and assume the model is correct, i.e. that the data are actually generated by this model. They also tend to break when the problem is badly conditioned (more features than samples).\n",
    "\n",
    "#### Multi-task Lasso\n",
    "\n",
    "The MultiTaskLasso is a linear model that estimates sparse coefﬁcients for multiple regression problems jointly: y isa2Darray,ofshape (n_samples, n_tasks). The constraint is that the selected features are the same for all the regression problems, also called tasks. \n",
    "\n",
    ">$\\min_{w}{\\frac{1}{2n_{samples}}||X W - y||^2_{Fro} + \\alpha||W||_{21}}$\n",
    "\n",
    "where Fro indicates the Frobenius norm\n",
    ">$||A||_{Fro} = \\sqrt{\\sum_{ij}{a^2_{ij}}}$\n",
    "\n",
    "and $l_1l_2$ reads\n",
    ">$||A||_{21} = \\sum_{i}{\\sqrt{\\sum_j{a^2_{ij}}}}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
