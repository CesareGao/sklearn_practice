{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.18.2 Plot Ridge coefﬁcients as a function of the regularization\n",
    "\n",
    "Shows the effect of collinearity in the coefﬁcients of an estimator. \n",
    "\n",
    "Ridge Regression is the estimator used in this example. Each color represents a different feature of the coefﬁcient vector, and this is displayed as a function of the regularization parameter. \n",
    "\n",
    "This example also shows the usefulness of applying Ridge regression to highly ill-conditioned matrices. For such matrices, a slight change in the target variable can cause huge variances in the calculated weights. In such cases, it is useful to set a certain regularization (alpha) to reduce this variation (noise). \n",
    "\n",
    "When alpha is very large, the regularization effect dominates the squared loss function and the coefﬁcients tend to zero. At the end of the path, as alpha tends toward zero and the solution tends towards the ordinary least squares, coefﬁcients exhibit big oscillations. In practise it is necessary to tune alpha in such a way that a balance is maintained between both.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "\n",
    "# X is the 10x10 Hilbert matrix\n",
    "X = 1./(np.arange(1,11) + np.arange(0,10)[:,np.newaxis])\n",
    "y = np.ones(10)\n",
    "\n",
    "n_alphas = 200\n",
    "alphas = np.logspace(-10, -2, n_alphas)\n",
    "\n",
    "coefs = []\n",
    "ridge = linear_model.Ridge(fit_intercept=False)\n",
    "for a in alphas:\n",
    "    ridge.alpha = a\n",
    "    ridge.fit(X,y)\n",
    "    coefs.append(ridge.coef_)\n",
    "\n",
    "# Display results\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas,coefs)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(ax.get_xlim()[::-1]) # reverse axis\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')\n",
    "plt.title('Ridge coefficients as a function of the regularization') \n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.18.13 Linear Regression Example\n",
    "\n",
    "This example uses the only the ﬁrst feature of the diabetes dataset, in order to illustrate a two-dimensional plot of this regression technique. The straight line can be seen in the plot, showing how linear regression attempts to draw a straight line that will best minimize the residual sum of squares between the observed responses in the dataset, and the responses predicted by the linear approximation. \n",
    "\n",
    "The coefﬁcients, the residual sum of squares and the variance score are also calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# load the diabetes dataset\n",
    "diabetes = datasets.load_diabetes()\n",
    "\n",
    "# Use only one feature\n",
    "diabetes_X = diabetes.data[:,np.newaxis,2]\n",
    "print(diabetes_X.shape)\n",
    "\n",
    "# Split the data into training/testing sets\n",
    "diabetes_X_train = diabetes_X[:-20]\n",
    "diabetes_X_test = diabetes_X[-20:]\n",
    "diabetes_y_train = diabetes.target[:-20]\n",
    "diabetes_y_test = diabetes.target[-20:]\n",
    "\n",
    "# Create linear regression object \n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the mdoel\n",
    "regr.fit(diabetes_X_train, diabetes_y_train)\n",
    "\n",
    "# Make prediction using the test set\n",
    "diabetes_y_pred = regr.predict(diabetes_X_test)\n",
    "\n",
    "# The coefficient\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: {:.2f}\".format(\n",
    "                mean_squared_error(diabetes_y_test,diabetes_y_pred)))\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print(\"Variance score: {:.2f}\".format(\n",
    "                r2_score(diabetes_y_test,diabetes_y_pred)))\n",
    "\n",
    "# Plot output\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(diabetes_X_test, diabetes_y_test, color='black')\n",
    "plt.plot(diabetes_X_test, diabetes_y_pred, color='blue',linewidth=3)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.18.23 Lasso and Elastic Net for Sparse Signals\n",
    "\n",
    "Estimates Lasso and Elastic-Net regression models on a manually generated sparse signal corrupted with an additive noise. Estimated coefﬁcients are compared with the ground-truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Generate some sparse data to play with \n",
    "np.random.seed(42)\n",
    "n_samples, n_features = 50, 100\n",
    "X = np.random.randn(n_samples,n_features)\n",
    "\n",
    "# Decreasing coef w. alternated signs for visualization\n",
    "idx = np.arange(n_features)\n",
    "coef = (-1)** idx * np.exp(-idx/10)\n",
    "coef[10:] = 0 # sparsify coef\n",
    "y = np.dot(X, coef)\n",
    "\n",
    "# Add noise\n",
    "y += 0.01 * np.random.normal(size=n_samples)\n",
    "\n",
    "n_samples = X.shape[0]\n",
    "X_train, y_train = X[:n_samples // 2], y[:n_samples // 2]\n",
    "X_test, y_test = X[n_samples // 2:], y[n_samples // 2:]\n",
    "\n",
    "# Lasso\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "alpha = 0.1\n",
    "lasso = Lasso(alpha=alpha)\n",
    "\n",
    "y_pred_lasso = lasso.fit(X_train, y_train).predict(X_test)\n",
    "r2_score_lasso = r2_score(y_test, y_pred_lasso)\n",
    "print(lasso)\n",
    "print(\"r^2 on test data : %f\" % r2_score_lasso)\n",
    "\n",
    "# ElasticNet\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "enet = ElasticNet(alpha=alpha, l1_ratio=0.7)\n",
    "y_pred_enet = enet.fit(X_train, y_train).predict(X_test)\n",
    "r2_score_enet = r2_score(y_test, y_pred_enet)\n",
    "print(enet)\n",
    "print(\"r^2 on test data : %f\" % r2_score_enet)\n",
    "\n",
    "m, s, _ = plt.stem(np.where(enet.coef_)[0],\n",
    "                   enet.coef_[enet.coef_ != 0],\n",
    "                   markerfmt='x',\n",
    "                   label='Elastic net coefficients')\n",
    "plt.setp([m, s], color='#2ca02c')\n",
    "m, s, _ = plt.stem(np.where(lasso.coef_)[0],\n",
    "                   lasso.coef_[lasso.coef_ != 0],\n",
    "                   markerfmt='x',\n",
    "                   label='Lasso coefficients')\n",
    "plt.setp([m, s], color='#ff7f0e')\n",
    "plt.stem(np.where(coef)[0],\n",
    "         coef[coef != 0],\n",
    "         markerfmt='bx',\n",
    "         label='True coefficients')\n",
    "plt.legend(loc='best')\n",
    "plt.title(\"Lasso $R^2$: %.3f, Elastic Net $R^2$: %.3f\" \n",
    "          % (r2_score_lasso, r2_score_enet))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
