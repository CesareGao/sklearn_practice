{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.18.1 Lasso path using LARS\n",
    "\n",
    "Computes Lasso Path along the regularization parameter using the LARS algorithm on the diabetes dataset. Each color represents a different feature of the coefﬁcient vector, and this is displayed as a function of the regularization parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn import datasets\n",
    "\n",
    "diabetes = datasets.load_diabetes() \n",
    "X = diabetes.data \n",
    "y = diabetes.target\n",
    "print(\"Computing regularization path using the LARS ...\")\n",
    "_, _, coefs = linear_model.lars_path(X, y, method='lasso', verbose=True)\n",
    "print(coefs.shape)\n",
    "xx = np.sum(np.abs(coefs.T), axis=1)\n",
    "print(xx.shape)\n",
    "xx /= xx[-1]\n",
    "plt.plot(xx, coefs.T)\n",
    "ymin, ymax = plt.ylim() \n",
    "plt.vlines(xx, ymin, ymax, linestyle='dashed') \n",
    "plt.xlabel('|coef| / max|coef|') \n",
    "plt.ylabel('Coefficients') \n",
    "plt.title('LASSO Path') \n",
    "plt.axis('tight') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.18.2 Plot Ridge coefﬁcients as a function of the regularization\n",
    "\n",
    "Shows the effect of collinearity in the coefﬁcients of an estimator. \n",
    "\n",
    "Ridge Regression is the estimator used in this example. Each color represents a different feature of the coefﬁcient vector, and this is displayed as a function of the regularization parameter. \n",
    "\n",
    "This example also shows the usefulness of applying Ridge regression to highly ill-conditioned matrices. For such matrices, a slight change in the target variable can cause huge variances in the calculated weights. In such cases, it is useful to set a certain regularization (alpha) to reduce this variation (noise). \n",
    "\n",
    "When alpha is very large, the regularization effect dominates the squared loss function and the coefﬁcients tend to zero. At the end of the path, as alpha tends toward zero and the solution tends towards the ordinary least squares, coefﬁcients exhibit big oscillations. In practise it is necessary to tune alpha in such a way that a balance is maintained between both.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "\n",
    "# X is the 10x10 Hilbert matrix\n",
    "X = 1./(np.arange(1,11) + np.arange(0,10)[:,np.newaxis])\n",
    "y = np.ones(10)\n",
    "\n",
    "n_alphas = 200\n",
    "alphas = np.logspace(-10, -2, n_alphas)\n",
    "\n",
    "coefs = []\n",
    "ridge = linear_model.Ridge(fit_intercept=False)\n",
    "for a in alphas:\n",
    "    ridge.alpha = a\n",
    "    ridge.fit(X,y)\n",
    "    coefs.append(ridge.coef_)\n",
    "\n",
    "# Display results\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas,coefs)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(ax.get_xlim()[::-1]) # reverse axis\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')\n",
    "plt.title('Ridge coefficients as a function of the regularization') \n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.18.8 Regularization path of L1- Logistic Regression\n",
    "\n",
    "Train l1-penalized logistic regression models on a binary classiﬁcation problem derived from the Iris dataset.\n",
    "\n",
    "The models are ordered from strongest regularized to least regularized. The 4 coefﬁcients of the models are collected and plotted as a “regularization path”: on the left-hand side of the ﬁgure (strong regularizers), all the coefﬁcients are exactly 0. When regularization gets progressively looser, coefﬁcients can get non-zero values one after the other. \n",
    "\n",
    "Here we choose the SAGA solver because it can efﬁciently optimize for the Logistic Regression loss with a nonsmooth, sparsity inducing l1 penalty. \n",
    "\n",
    "Also note that we set a low value for the tolerance to make sure that the model has converged before collecting the coefﬁcients. \n",
    "\n",
    "We also use warm_start=True which means that the coefﬁcients of the models are reused to initialize the next model ﬁt to speed-up the computation of the full-path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn import linear_model \n",
    "from sklearn import datasets \n",
    "from sklearn.svm import l1_min_c # get the lower bound to avoid zero coef\n",
    "\n",
    "iris = datasets.load_iris() \n",
    "X = iris.data \n",
    "y = iris.target\n",
    "X = X[y != 2] \n",
    "y = y[y != 2] # remove the third class to convert it to a binary problem\n",
    "\n",
    "# Demo path functions\n",
    "cs = l1_min_c(X, y, loss='log') * np.logspace(0,7,16)\n",
    "print(\"Computing regularization path ...\")\n",
    "start = time()\n",
    "clf = linear_model.LogisticRegression(penalty='l1', solver='saga',\n",
    "                                      tol=1e-6, max_iter=int(1e6),\n",
    "                                      warm_start=True)\n",
    "coefs_ = []\n",
    "for c in cs:\n",
    "    clf.set_params(C=c)\n",
    "    clf.fit(X,y)\n",
    "    coefs_.append(clf.coef_.ravel().copy())\n",
    "print(\"This took %0.3fs\" % (time() - start))\n",
    "coefs_ = np.array(coefs_)\n",
    "plt.plot(np.log10(cs), coefs_, marker='o')\n",
    "plt.xlabel('log(C)')\n",
    "plt.ylabel('Coefficients')\n",
    "plt.title('logistic Regression Path')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.18.13 Linear Regression Example\n",
    "\n",
    "This example uses the only the ﬁrst feature of the diabetes dataset, in order to illustrate a two-dimensional plot of this regression technique. The straight line can be seen in the plot, showing how linear regression attempts to draw a straight line that will best minimize the residual sum of squares between the observed responses in the dataset, and the responses predicted by the linear approximation. \n",
    "\n",
    "The coefﬁcients, the residual sum of squares and the variance score are also calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# load the diabetes dataset\n",
    "diabetes = datasets.load_diabetes()\n",
    "\n",
    "# Use only one feature\n",
    "diabetes_X = diabetes.data[:,np.newaxis,2]\n",
    "print(diabetes_X.shape)\n",
    "\n",
    "# Split the data into training/testing sets\n",
    "diabetes_X_train = diabetes_X[:-20]\n",
    "diabetes_X_test = diabetes_X[-20:]\n",
    "diabetes_y_train = diabetes.target[:-20]\n",
    "diabetes_y_test = diabetes.target[-20:]\n",
    "\n",
    "# Create linear regression object \n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the mdoel\n",
    "regr.fit(diabetes_X_train, diabetes_y_train)\n",
    "\n",
    "# Make prediction using the test set\n",
    "diabetes_y_pred = regr.predict(diabetes_X_test)\n",
    "\n",
    "# The coefficient\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: {:.2f}\".format(\n",
    "                mean_squared_error(diabetes_y_test,diabetes_y_pred)))\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print(\"Variance score: {:.2f}\".format(\n",
    "                r2_score(diabetes_y_test,diabetes_y_pred)))\n",
    "\n",
    "# Plot output\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(diabetes_X_test, diabetes_y_test, color='black')\n",
    "plt.plot(diabetes_X_test, diabetes_y_pred, color='blue',linewidth=3)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.18.18 Joint feature selection with multi-task Lasso\n",
    "\n",
    "The multi-task lasso allows to ﬁt multiple regression problems jointly enforcing the selected features to be the same across tasks. This example simulates sequential measurements, each task is a time instant, and the relevant features vary in amplitude over time while being the same. The multi-task lasso imposes that features that are selected at one time point are select for all time point. This makes feature selection by the Lasso more stable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.linear_model import MultiTaskLasso, Lasso\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "# Generate some 2D coefficients with sine waves with random frequency and \n",
    "# phase\n",
    "n_samples, n_features, n_tasks = 100, 30, 40\n",
    "n_relevant_features = 5\n",
    "coef = np.zeros((n_tasks,n_features))\n",
    "times = np.linspace(0, 2*np.pi, n_tasks)\n",
    "for k in range(n_relevant_features):\n",
    "    coef[:,k] = np.sin((1. + rng.randn(1)) * times + 3*rng.randn(1))\n",
    "X = rng.randn(n_samples,n_features)\n",
    "Y = np.dot(X, coef.T) + rng.randn(n_samples, n_tasks)\n",
    "coef_lasso_ = np.array([Lasso(alpha=.5).fit(X,y).coef_ for y in Y.T])\n",
    "coef_multi_task_lasso_ = MultiTaskLasso(alpha=1.).fit(X,Y).coef_\n",
    "\n",
    "# Plot support and time series \n",
    "fig = plt.figure(figsize=(8, 5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.spy(coef_lasso_)\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Time (or task)')\n",
    "plt.text(10,5,'Lasso')\n",
    "plt.subplot(1,2,2)\n",
    "plt.spy(coef_multi_task_lasso_)\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Time (or task)')\n",
    "plt.text(10, 5, 'MultiTaskLasso')\n",
    "fig.suptitle('Coefficient non-zero location')\n",
    "fig.show()\n",
    "\n",
    "feature_to_plot = 0\n",
    "plt.figure() \n",
    "lw = 2\n",
    "plt.plot(coef[:, feature_to_plot], color='seagreen', linewidth=lw,\n",
    "         label='Ground truth') \n",
    "plt.plot(coef_lasso_[:, feature_to_plot], color='cornflowerblue', linewidth=lw,\n",
    "         label='Lasso')\n",
    "plt.plot(coef_multi_task_lasso_[:, feature_to_plot], color='gold', linewidth=lw, \n",
    "         label='MultiTaskLasso') \n",
    "plt.legend(loc='lower left') \n",
    "plt.axis('tight') \n",
    "plt.ylim([-1.1, 1.1]) \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.18.20 Orthogonal Matching Pursuit\n",
    "\n",
    "Using orthogonal matching pursuit for recovering as parse signal from a noisy measurement encoded with a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.linear_model import OrthogonalMatchingPursuit, \\\n",
    "                                 OrthogonalMatchingPursuitCV\n",
    "from sklearn.datasets import make_sparse_coded_signal\n",
    "\n",
    "n_components, n_features = 512, 100\n",
    "n_nonzero_coefs = 17\n",
    "\n",
    "# Generate data\n",
    "y, X, w = make_sparse_coded_signal(n_samples=1,\n",
    "                                   n_components=n_components,\n",
    "                                   n_features=n_features,\n",
    "                                   n_nonzero_coefs=n_nonzero_coefs,\n",
    "                                   random_state=0)\n",
    "idx, = w.nonzero()\n",
    "\n",
    "# distort the clean signal\n",
    "y_noisy = y + 0.05 * np.random.randn(len(y))\n",
    "\n",
    "# plot the sparse signal\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.subplot(4,1,1)\n",
    "plt.xlim(0,512)\n",
    "plt.title(\"Sparse signal\")\n",
    "plt.stem(idx, w[idx])\n",
    "\n",
    "# plot the noise-free reconstruction\n",
    "omp = OrthogonalMatchingPursuit(n_nonzero_coefs=n_nonzero_coefs)\n",
    "omp.fit(X, y)\n",
    "coef = omp.coef_\n",
    "idx_r, = coef.nonzero()\n",
    "plt.subplot(4,1,2)\n",
    "plt.xlim(0,512)\n",
    "plt.title(\"Recovered signal from noise-free measurements\")\n",
    "plt.stem(idx_r, coef[idx_r])\n",
    "\n",
    "# plot the noisy reconstruction\n",
    "omp.fit(X,y_noisy)\n",
    "coef = omp.coef_\n",
    "idx_r, = coef.nonzero()\n",
    "plt.subplot(4,1,3)\n",
    "plt.xlim(0,512)\n",
    "plt.title(\"Recovered signal from noisy measurements\")\n",
    "plt.stem(idx_r, coef[idx_r])\n",
    "          \n",
    "# plot the noisy reconstruction with number of non-zeros set by CV\n",
    "omp_cv = OrthogonalMatchingPursuitCV(cv=5)\n",
    "omp_cv.fit(X, y_noisy) \n",
    "coef = omp_cv.coef_\n",
    "idx_r, = coef.nonzero() \n",
    "plt.subplot(4, 1, 4) \n",
    "plt.xlim(0, 512)\n",
    "plt.title(\"Recovered signal from noisy measurements with CV\") \n",
    "plt.stem(idx_r, coef[idx_r])\n",
    "\n",
    "plt.subplots_adjust(0.06, 0.04, 0.94, 0.90, 0.20, 0.38)\n",
    "plt.suptitle('Sparse signal recovery with Orthogonal Matching Pursuit',\n",
    "             fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.18.21 MNIST classﬁﬁcation using multinomial logistic + L1\n",
    "\n",
    "Here we ﬁt a multinomial logistic regression with L1 penalty on a subset of the MNIST digits classiﬁcation task. We use the SAGA algorithm for this purpose: this a solver that is fast when the number of samples is signiﬁcantly larger than the number of features and is able to ﬁnely optimize non-smooth objective functions which is the case with the l1-penalty. Test accuracy reaches > 0.8, while weight vectors remains sparse and therefore more easily interpretable. \n",
    "\n",
    "Note that this accuracy of this l1-penalized linear model is signiﬁcantly below what can be reached by an l2-penalized linear model or a non-linear multi-layer perceptron model on this dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# Turn down for faster convergence\n",
    "t0 = time.time()\n",
    "train_samples = 5000\n",
    "\n",
    "# Load data from https://www.openml.org/d/554\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "random_state = check_random_state(0)\n",
    "permutation = random_state.permutation(X.shape[0])\n",
    "X = X[permutation] \n",
    "y = y[permutation]\n",
    "X = X.reshape((X.shape[0], -1)) # flatten the picture\n",
    "X_train, X_test, y_train, y_test = train_test_split( \n",
    "    X, y, train_size=train_samples, test_size=10000)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Turn up tolerance for faster convergence\n",
    "clf = LogisticRegression(C=50/train_samples,\n",
    "                         multi_class='multinomial',\n",
    "                         penalty='l1',\n",
    "                         solver='saga',\n",
    "                         tol=0.1)\n",
    "clf.fit(X_train, y_train)\n",
    "sparsity = np.mean(clf.coef_==0) * 100\n",
    "score = clf.score(X_test, y_test)\n",
    "print('Best C % .4f' % clf.get_params()['C']) \n",
    "print(\"Sparsity with L1 penalty: %.2f%%\" % sparsity) \n",
    "print(\"Test score with L1 penalty: %.4f\" % score)\n",
    "coef = clf.coef_.copy()\n",
    "plt.figure(figsize=(10,5))\n",
    "scale = np.abs(coef).max()\n",
    "for i in range(10):\n",
    "    l1_plot = plt.subplot(2,5,i+1)\n",
    "    l1_plot.imshow(coef[i].reshape(28,28), interpolation='nearest',\n",
    "                   cmap=plt.cm.RdBu, vmin=-scale, vmax=scale)\n",
    "    l1_plot.set_xticks(())\n",
    "    l1_plot.set_yticks(())\n",
    "    l1_plot.set_xlabel('Class %i' % i)\n",
    "plt.suptitle('Classification vector for...')\n",
    "run_time = time.time() - t0 \n",
    "print('Example run in %.3f s' % run_time) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.18.23 Lasso and Elastic Net for Sparse Signals\n",
    "\n",
    "Estimates Lasso and Elastic-Net regression models on a manually generated sparse signal corrupted with an additive noise. Estimated coefﬁcients are compared with the ground-truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Generate some sparse data to play with \n",
    "np.random.seed(42)\n",
    "n_samples, n_features = 50, 100\n",
    "X = np.random.randn(n_samples,n_features)\n",
    "\n",
    "# Decreasing coef w. alternated signs for visualization\n",
    "idx = np.arange(n_features)\n",
    "coef = (-1)** idx * np.exp(-idx/10)\n",
    "coef[10:] = 0 # sparsify coef\n",
    "y = np.dot(X, coef)\n",
    "\n",
    "# Add noise\n",
    "y += 0.01 * np.random.normal(size=n_samples)\n",
    "\n",
    "n_samples = X.shape[0]\n",
    "X_train, y_train = X[:n_samples // 2], y[:n_samples // 2]\n",
    "X_test, y_test = X[n_samples // 2:], y[n_samples // 2:]\n",
    "\n",
    "# Lasso\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "alpha = 0.1\n",
    "lasso = Lasso(alpha=alpha)\n",
    "\n",
    "y_pred_lasso = lasso.fit(X_train, y_train).predict(X_test)\n",
    "r2_score_lasso = r2_score(y_test, y_pred_lasso)\n",
    "print(lasso)\n",
    "print(\"r^2 on test data : %f\" % r2_score_lasso)\n",
    "\n",
    "# ElasticNet\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "enet = ElasticNet(alpha=alpha, l1_ratio=0.7)\n",
    "y_pred_enet = enet.fit(X_train, y_train).predict(X_test)\n",
    "r2_score_enet = r2_score(y_test, y_pred_enet)\n",
    "print(enet)\n",
    "print(\"r^2 on test data : %f\" % r2_score_enet)\n",
    "\n",
    "m, s, _ = plt.stem(np.where(enet.coef_)[0],\n",
    "                   enet.coef_[enet.coef_ != 0],\n",
    "                   markerfmt='x',\n",
    "                   label='Elastic net coefficients')\n",
    "plt.setp([m, s], color='#2ca02c')\n",
    "m, s, _ = plt.stem(np.where(lasso.coef_)[0],\n",
    "                   lasso.coef_[lasso.coef_ != 0],\n",
    "                   markerfmt='x',\n",
    "                   label='Lasso coefficients')\n",
    "plt.setp([m, s], color='#ff7f0e')\n",
    "plt.stem(np.where(coef)[0],\n",
    "         coef[coef != 0],\n",
    "         markerfmt='bx',\n",
    "         label='True coefficients')\n",
    "plt.legend(loc='best')\n",
    "plt.title(\"Lasso $R^2$: %.3f, Elastic Net $R^2$: %.3f\" \n",
    "          % (r2_score_lasso, r2_score_enet))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.18.25 Plot multinomial and One-vs-Rest Logistic Regression\n",
    "\n",
    "Plot decision surface of multinomial and One-vs-Rest Logistic Regression. The hyperplanes corresponding to the three One-vs-Rest (OVR) classiﬁers are represented by the dashed lines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# make 3-class dataset for classification \n",
    "centers = [[-5, 0], [0, 1.5], [5, -1]]\n",
    "X, y = make_blobs(n_samples=1000, centers=centers, random_state=40) \n",
    "transformation = [[0.4, 0.2], [-0.4, 1.2]] \n",
    "X = np.dot(X, transformation)\n",
    "for multi_class in ('multinomial', 'ovr'):\n",
    "    clf = LogisticRegression(solver='sag',\n",
    "                             max_iter=100,\n",
    "                             random_state=42,\n",
    "                             multi_class=multi_class).fit(X, y)\n",
    "    # print the training scores\n",
    "    print(\"training score : %.3f (%s)\" % (clf.score(X, y), multi_class))\n",
    "    # create a mesh to plot in\n",
    "    h = .02 # step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 \n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), \n",
    "                 np.arange(y_min, y_max, h))\n",
    "#     print(xx.shape,xx.ravel().shape)\n",
    "    # Plot the decision boundary. For that, we will assign a color to each \n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max]. \n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape) \n",
    "    plt.figure() \n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "    plt.title(\"Decision surface of LogisticRegression (%s)\" % multi_class) \n",
    "    plt.axis('tight')\n",
    "\n",
    "    # Plot also the training points \n",
    "    colors = \"bry\" # distinct blue, red, yellow\n",
    "    for i, color in zip(clf.classes_, colors): \n",
    "        idx = np.where(y == i)\n",
    "        plt.scatter(X[idx, 0], X[idx, 1], c=color, cmap=plt.cm.Paired, \n",
    "                    edgecolor='black', s=20)\n",
    "\n",
    "    # Plot the three one-against-all classifiers \n",
    "    xmin, xmax = plt.xlim() \n",
    "    ymin, ymax = plt.ylim()\n",
    "    coef = clf.coef_\n",
    "    intercept = clf.intercept_\n",
    " \n",
    "    def plot_hyperplane(c, color):\n",
    "        def line(x0): \n",
    "            return (-(x0 * coef[c, 0]) - intercept[c]) / coef[c, 1] \n",
    "        plt.plot([xmin, xmax], [line(xmin), line(xmax)], \n",
    "                 ls=\"--\", color=color)\n",
    "    for i, color in zip(clf.classes_, colors): \n",
    "        plot_hyperplane(i, color)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  5.18.27 L1 Penalty and Sparsity in Logistic Regression\n",
    "\n",
    "Comparison of the sparsity (percentage of zero coefﬁcients) of solutions when L1, L2 and Elastic-Net penalty are used for different values of C. We can see that large values of C give more freedom to the model. Conversely,smaller values of C constrain the model more. In the L1 penalty case, this leads to sparser solutions. As expected, the Elastic-Net penalty sparsity is between that of L1 and L2. \n",
    "\n",
    "We classify 8x8 images of digits into two classes: 0-4 against 5-9. The visualization shows coefﬁcients of the models for varying C.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "X, y = digits.data, digits.target \n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# classify small against large digits\n",
    "y = (y>4).astype(np.int)\n",
    "l1_ratio = 0.5 # L1 weight in the Elastic-Net regularization\n",
    "fig, axes = plt.subplots(3,3)\n",
    "\n",
    "for i, (C, axes_row) in enumerate(zip((1,0.1,0.01),axes)):\n",
    "    # turn down tolerance for short training time\n",
    "    clf_l1_LR = LogisticRegression(C=C, penalty='l1', tol=0.01, solver='saga')\n",
    "    clf_l2_LR = LogisticRegression(C=C, penalty='l2', tol=0.01, solver='saga')\n",
    "    clf_en_LR = LogisticRegression(C=C, penalty='elasticnet', solver='saga',\n",
    "                                   l1_ratio=l1_ratio, tol=0.01)\n",
    "    clf_l1_LR.fit(X,y)\n",
    "    clf_l2_LR.fit(X, y) \n",
    "    clf_en_LR.fit(X, y)\n",
    "    coef_l1_LR = clf_l1_LR.coef_.ravel()\n",
    "    coef_l2_LR = clf_l2_LR.coef_.ravel()\n",
    "    coef_en_LR = clf_en_LR.coef_.ravel()\n",
    "    # coef_l1_LR contains zeros due to the \n",
    "    # L1 sparsity inducing norm\n",
    "    sparsity_l1_LR = np.mean(coef_l1_LR == 0) * 100\n",
    "    sparsity_l2_LR = np.mean(coef_l2_LR == 0) * 100 \n",
    "    sparsity_en_LR = np.mean(coef_en_LR == 0) * 100\n",
    "    print(\"C=%.2f\" % C)\n",
    "    print(\"{:<40} {:.2f}%\".format(\"Sparsity with L1 penalty:\", sparsity_l1_LR)) \n",
    "    print(\"{:<40} {:.2f}%\".format(\"Sparsity with Elastic-Net penalty:\", \n",
    "                                  sparsity_en_LR))\n",
    "    print(\"{:<40} {:.2f}%\".format(\"Sparsity with L2 penalty:\", sparsity_l2_LR)) \n",
    "    print(\"{:<40} {:.2f}\".format(\"Score with L1 penalty:\", \n",
    "                                 clf_l1_LR.score(X, y))) \n",
    "    print(\"{:<40} {:.2f}\".format(\"Score with Elastic-Net penalty:\", \n",
    "                                 clf_en_LR.score(X, y))) \n",
    "    print(\"{:<40} {:.2f}\".format(\"Score with L2 penalty:\", \n",
    "                                 clf_l2_LR.score(X, y)))\n",
    "    if i == 0:\n",
    "        axes_row[0].set_title(\"L1 penalty\") \n",
    "        axes_row[1].set_title(\"Elastic-Net\\nl1_ratio = %s\" % l1_ratio) \n",
    "        axes_row[2].set_title(\"L2 penalty\")\n",
    "    for ax, coefs in zip(axes_row, [coef_l1_LR, coef_en_LR, coef_l2_LR]):\n",
    "        ax.imshow(np.abs(coefs.reshape(8, 8)), interpolation='nearest', \n",
    "                  cmap='binary', vmax=1, vmin=0) \n",
    "        ax.set_xticks(()) \n",
    "        ax.set_yticks(())\n",
    "    axes_row[0].set_ylabel('C = %s' % C)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.18.28 Lasso and Elastic Net\n",
    "Lasso and elastic net (L1 and L2 penalisation) implemented using a coordinate descent. The coefﬁcients can be forced to be positive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.linear_model import lasso_path, enet_path\n",
    "from sklearn import datasets\n",
    "\n",
    "diabetes = datasets.load_diabetes() \n",
    "X = diabetes.data \n",
    "y = diabetes.target\n",
    "X /= X.std(axis=0) # Standardize data (easier to set the l1_ratio parameter)\n",
    "print(X.shape)\n",
    "# Compute paths\n",
    "eps = 5e-3 # the smaller it is the longer is the path\n",
    "print(\"Computing regularization path using the lasso...\")\n",
    "alphas_lasso, coefs_lasso, _ = lasso_path(X, y, eps, fit_intercept=False)\n",
    "print(\"Computing regularization path using the positive lasso...\") \n",
    "alphas_positive_lasso, coefs_positive_lasso, _ = lasso_path( \n",
    "    X, y, eps, positive=True, fit_intercept=False) \n",
    "print(\"Computing regularization path using the elastic net...\") \n",
    "alphas_enet, coefs_enet, _ = enet_path(\n",
    "    X, y, eps=eps, l1_ratio=0.8, fit_intercept=False)\n",
    "print(\"Computing regularization path using the positive elastic net...\") \n",
    "alphas_positive_enet, coefs_positive_enet, _ = enet_path( \n",
    "    X, y, eps=eps, l1_ratio=0.8, positive=True, fit_intercept=False)\n",
    "\n",
    "# Display results\n",
    "plt.figure(1)\n",
    "colors = cycle(['b', 'r', 'g', 'c', 'k'])\n",
    "neg_log_alphas_lasso = -np.log10(alphas_lasso)\n",
    "neg_log_alphas_enet = -np.log10(alphas_enet)\n",
    "for coef_l, coef_e, c in zip(coefs_lasso, coefs_enet, colors):\n",
    "    l1 = plt.plot(neg_log_alphas_lasso, coef_l, c=c) \n",
    "    l2 = plt.plot(neg_log_alphas_enet, coef_e, linestyle='--', c=c)\n",
    "plt.xlabel('-Log(alpha)')\n",
    "plt.ylabel('coefficients')\n",
    "plt.title('Lasso and Elastic-Net Paths')\n",
    "plt.legend((l1[-1], l2[-1]), ('Lasso', 'Elastic-Net'), loc='lower left') \n",
    "plt.axis('tight')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(2)\n",
    "neg_log_alphas_positive_lasso = -np.log10(alphas_positive_lasso)\n",
    "for coef_l, coef_pl, c in zip(coefs_lasso, coefs_positive_lasso, colors):\n",
    "    l1 = plt.plot(neg_log_alphas_lasso, coef_l, c=c)\n",
    "    l2 = plt.plot(neg_log_alphas_positive_lasso, coef_pl, linestyle='--', c=c)\n",
    "plt.xlabel('-Log(alpha)')\n",
    "plt.ylabel('coefficients')\n",
    "plt.title('Lasso and positive Lasso')\n",
    "plt.legend((l1[-1], l2[-1]), ('Lasso', 'positive Lasso'), loc='lower left') \n",
    "plt.axis('tight')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(3)\n",
    "neg_log_alphas_positive_enet = -np.log10(alphas_positive_enet)\n",
    "for (coef_e, coef_pe, c) in zip(coefs_enet, coefs_positive_enet, colors):\n",
    "    l1 = plt.plot(neg_log_alphas_enet, coef_e, c=c) \n",
    "    l2 = plt.plot(neg_log_alphas_positive_enet, coef_pe, linestyle='--', c=c)\n",
    "plt.xlabel('-Log(alpha)')\n",
    "plt.ylabel('coefficients')\n",
    "plt.title('Elastic-Net and positive Elastic-Net')\n",
    "plt.legend((l1[-1], l2[-1]), ('Elastic-Net', 'positive Elastic-Net'),  loc='lower left') \n",
    "plt.axis('tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  5.18.29 Automatic Relevance Determination Regression (ARD)\n",
    "\n",
    "Fit regression model with Bayesian Ridge Regression.\n",
    "\n",
    "Compared to the OLS (ordinary least squares) estimator, the coefﬁcient weights are slightly shifted toward zeros, which stabilises them. \n",
    "\n",
    "The histogram of the estimated weights is very peaked, as a sparsity-inducing prior is implied on the weights. \n",
    "\n",
    "The estimation of the model is done by iteratively maximizing the marginal log-likelihood of the observations. \n",
    "\n",
    "We also plot predictions and uncertainties for ARD for one dimensional regression using polynomial feature expansion. Note the uncertainty starts going up on the right side of the plot. This is because these test samples are outside of the range of the training samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import ARDRegression, LinearRegression\n",
    "\n",
    "# Generating simulated data with Gaussian weights\n",
    "# Parameters of the example \n",
    "np.random.seed(0)\n",
    "n_samples, n_features = 100, 100\n",
    "# Create Gaussian data\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "# Create weights with a precision lambda_ of 4. \n",
    "lambda_ = 4. \n",
    "w = np.zeros(n_features)\n",
    "# Only keep 10 weights of interest\n",
    "relevant_features = np.random.randint(0, n_features, 10) \n",
    "for i in relevant_features: \n",
    "    w[i] = stats.norm.rvs(loc=0, scale=1. / np.sqrt(lambda_)) \n",
    "# Create noise with a precision alpha of 50. \n",
    "alpha_ = 50.\n",
    "noise = stats.norm.rvs(loc=0, scale=1. / np.sqrt(alpha_), size=n_samples) \n",
    "# Create the target \n",
    "y = np.dot(X, w) + noise\n",
    "\n",
    "# Fit the ARD Regression\n",
    "clf = ARDRegression(compute_score=True)\n",
    "clf.fit(X,y)\n",
    "\n",
    "ols = LinearRegression()\n",
    "ols.fit(X,y)\n",
    "\n",
    "# Plot the true weights, the estimated weights, the histogram of \n",
    "# the weights, and predictions with standard deviations\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.title(\"Weights of the mdoel\")\n",
    "plt.plot(clf.coef_, color='darkblue', linestyle='-', linewidth=2,\n",
    "         label='ARD estimate')\n",
    "plt.plot(ols.coef_, color='yellowgreen', linestyle=':', linewidth=2,\n",
    "         label='OLS estimate')\n",
    "plt.plot(w, color='orange', linestyle='-', linewidth=1,\n",
    "         label=\"Ground Truth\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Value of the weights\")\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.18.30 Bayesian Ridge Regression\n",
    "\n",
    "Computes a Bayesian Ridge Regression on a synthetic dataset. \n",
    "\n",
    "See Bayesian Ridge Regression for more information on the regressor. \n",
    "\n",
    "Compared to the OLS (ordinary least squares) estimator, the coefﬁcient weights are slightly shifted toward zeros, which stabilises them.\n",
    "\n",
    "As the prior on the weights is a Gaussian prior, the histogram of the estimated weights is Gaussian. \n",
    "\n",
    "The estimation of the model is done by iteratively maximizing the marginal log-likelihood of the observations. \n",
    "\n",
    "We also plot predictions and uncertainties for Bayesian Ridge Regression for one dimensional regression using polynomial feature expansion. Note the uncertainty starts going up on the right side of the plot. This is because these test samples are outside of the range of the training samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import BayesianRidge, LinearRegression\n",
    "\n",
    "# Generated simulated data with Gaussian data\n",
    "np.random.seed(0)\n",
    "n_samples, n_features = 100, 100\n",
    "X = np.random.randn(n_samples, n_features) # Create Gaussian data \n",
    "# Create weights with a precision lambda_ of 4.\n",
    "lambda_ = 4. \n",
    "w = np.zeros(n_features)\n",
    "# Only keep 10 weights of interest\n",
    "relevant_features = np.random.randint(0, n_features, 10)\n",
    "for i in relevant_features:\n",
    "    w[i] = stats.norm.rvs(loc=0, scale=1./np.sqrt(lambda_))\n",
    "# Create noise with a precision alpha of 50. \n",
    "alpha_ = 50. \n",
    "noise = stats.norm.rvs(loc=0, scale=1. / np.sqrt(alpha_), size=n_samples) \n",
    "# Create the target\n",
    "y = np.dot(X, w) + noise\n",
    "\n",
    "# Fit the Bayesian Ridge Regression and an OLS for comparison\n",
    "clf = BayesianRidge(compute_score=True) \n",
    "clf.fit(X, y)\n",
    "ols = LinearRegression() \n",
    "ols.fit(X, y)\n",
    "\n",
    "# Plot true weights, estimated weights, histogram of the weights, and \n",
    "# predictions with standard deviations\n",
    "lw = 2 \n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.title(\"Weights of the model\")\n",
    "plt.plot(clf.coef_, color='lightgreen', linewidth=lw, \n",
    "         label=\"Bayesian Ridge estimate\") \n",
    "plt.plot(w, color='gold', linewidth=1, label=\"Ground truth\")\n",
    "plt.plot(ols.coef_, color='navy', linestyle='--', label=\"OLS estimate\") \n",
    "plt.xlabel(\"Features\") \n",
    "plt.ylabel(\"Values of the weights\")\n",
    "plt.legend(loc=\"best\", prop=dict(size=12))\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 5)) \n",
    "plt.title(\"Histogram of the weights\")\n",
    "plt.hist(clf.coef_, bins=n_features, color='gold', log=True,\n",
    "         edgecolor='black') \n",
    "plt.scatter(clf.coef_[relevant_features], np.full(len(relevant_features), 5.),\n",
    "            color='navy', label=\"Relevant features\",s=4) \n",
    "plt.ylabel(\"Features\")\n",
    "plt.xlabel(\"Values of the weights\") \n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 5)) \n",
    "plt.title(\"Marginal log-likelihood\")\n",
    "plt.plot(clf.scores_, color='navy', linewidth=lw) \n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.show()\n",
    "\n",
    "# Plotting some predictions for polynomial regression \n",
    "def f(x, noise_amount):\n",
    "    y = np.sqrt(x) * np.sin(x)\n",
    "    noise = np.random.normal(0,1,len(x))\n",
    "    return y + noise_amount * noise\n",
    "degree = 10\n",
    "X = np.linspace(0,10,100)\n",
    "y = f(X, noise_amount=0.1)\n",
    "clf_poly = BayesianRidge()\n",
    "clf_poly.fit(np.vander(X, degree), y)\n",
    "X_plot = np.linspace(0, 11, 25)\n",
    "y_plot = f(X_plot, noise_amount=0)\n",
    "y_mean, y_std = clf_poly.predict(np.vander(X_plot, degree), return_std=True)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.errorbar(X_plot, y_mean, y_std, color='navy',\n",
    "             label=\"Polynomial Bayesian Ridge Regression\", linewidth=lw)\n",
    "plt.plot(X_plot, y_plot, color='gold', linewidth=lw,label=\"Ground Truth\")\n",
    "plt.ylabel(\"Output y\")\n",
    "plt.xlabel(\"Feature X\")\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.18.31 Lasso model selection: Cross-Validation / AIC / BIC\n",
    "Use the Akaike information criterion (AIC), the Bayes Information criterion (BIC) and cross-validation to select an optimal value of the regularization parameter alpha of the Lasso estimator. \n",
    "\n",
    "Results obtained with LassoLarsIC are based on AIC/BIC criteria.\n",
    "\n",
    "Information-criterion based model selection is very fast, but it relies on a proper estimation of degrees of freedom, are derived for large samples (asymptotic results) and assume the model is correct,i.e. that the data are actually generated by this model. They also tend to break when the problem is badly conditioned (more features than samples).\n",
    "\n",
    "For cross-validation, we use 20-fold with 2 algorithms to compute the Lassopath: coordinate descent,a simple mented by the LassoCV class, and Lars (least angle regression) as implemented by the LassoLarsCV class. Both algorithms give roughly the same results. They differ with regards to their execution speed and sources of numerical errors.\n",
    "\n",
    "Lars computes a path solution only for each kink in the path. As a result,it is very efﬁcient when there are only of few kinks, which is the case if there are few features or samples. Also, it is able to compute the full path without setting any meta parameter. On the opposite, coordinate descent compute the path points on a pre-speciﬁed grid (here we use thedefault). Thus it is more efﬁcient if the number of grid points is smaller than the number of kinks in the path. Such a strategy can be interesting if the number of features is really large and there are enough samples to select a large amount. In terms of numerical errors, for heavily correlated variables, Lars will accumulate more errors, while the coordinate descent algorithm will only sample the path on a grid. \n",
    "\n",
    "Note how the optimal value of alpha varies for each fold. This illustrates why nested-cross validation is necessary when trying to evaluate the performance of a method for which a parameter is chosen by cross-validation: this choice of parameter may not be optimal for unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LassoCV, LassoLarsCV, LassoLarsIC\n",
    "from sklearn import datasets\n",
    "\n",
    "# This is to avoid division by zero while doing np.log10\n",
    "EPSILON = 1e-4\n",
    "\n",
    "diabetes = datasets.load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "rng = np.random.RandomState(42)\n",
    "X = np.c_[X, rng.randn(X.shape[0],14)] # add some bad features\n",
    "\n",
    "# normalize data as done by Lars to allow for comparasion\n",
    "X /= np.sqrt(np.sum(X**2, axis=0))\n",
    "\n",
    "# LassoLarsIC: least angle regression with BIC/AIC criterion\n",
    "model_bic = LassoLarsIC(criterion='bic')\n",
    "t1 = time.time()\n",
    "model_bic.fit(X,y)\n",
    "t_bic = time.time() - t1\n",
    "alpha_bic_ = model_bic.alpha_\n",
    "\n",
    "model_aic = LassoLarsIC(criterion='aic')\n",
    "model_aic.fit(X,y)\n",
    "alpha_aic_ = model_aic.alpha_\n",
    "\n",
    "def plot_ic_criterion(model, name, color):\n",
    "    alpha_ = model.alpha_ + EPSILON\n",
    "    alphas_ = model.alphas_ + EPSILON\n",
    "    criterion_ = model.criterion_\n",
    "    plt.plot(-np.log10(alphas_), criterion_, '--', color=color,\n",
    "             linewidth=3, label='{} criterion'.format(name))\n",
    "    plt.axvline(-np.log10(alpha_), color=color, linewidth=3,\n",
    "                label='alpha: {} estimate'.format(name))\n",
    "    plt.xlabel('-log(alpha)')\n",
    "    plt.ylabel('criterion')\n",
    "\n",
    "plt.figure()\n",
    "plot_ic_criterion(model_aic, 'AIC', 'b')\n",
    "plot_ic_criterion(model_bic, 'BIC', 'r')\n",
    "plt.legend()\n",
    "plt.title('Information-criterion for model selection (training time %.3fs)'\n",
    "          % t_bic)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# LassoCV: coordinate descent\n",
    "\n",
    "# Compute path\n",
    "print('Computing regularization path using the coordinate descent lasso...')\n",
    "t1 = time.time()\n",
    "model = LassoCV(cv=20).fit(X,y)\n",
    "t_lasso_cv = time.time() - t1\n",
    "\n",
    "# Display results\n",
    "m_log_alphas = -np.log10(model.alphas_ + EPSILON)\n",
    "\n",
    "plt.figure()\n",
    "ymin, ymax = 2300, 3800\n",
    "plt.plot(m_log_alphas, model.mse_path_,':')\n",
    "# print(model.mse_path_.shape)\n",
    "plt.plot(m_log_alphas, model.mse_path_.mean(axis=-1),'k',\n",
    "         label='Average across the folds', linewidth=2)\n",
    "plt.axvline(-np.log10(model.alpha_ + EPSILON), linestyle='--', color='k',\n",
    "            label='alpha: CV estimate')\n",
    "plt.legend()\n",
    "plt.xlabel('-log(alpha)') \n",
    "plt.ylabel('Mean square error')\n",
    "plt.title('Mean square error on each fold: coordinate descent '\n",
    "          '(train time: %.2fs)' % t_lasso_cv) \n",
    "plt.axis('tight')\n",
    "plt.ylim(ymin, ymax)\n",
    "plt.show()\n",
    "\n",
    "# LassoLarsCV: least angle regression\n",
    "print(\"Computing regularization path using the Lars lasso...\") \n",
    "t1 = time.time() \n",
    "model = LassoLarsCV(cv=20).fit(X, y)\n",
    "t_lasso_lars_cv = time.time() - t1\n",
    "\n",
    "# Display results\n",
    "m_log_alphas = -np.log10(model.cv_alphas_ + EPSILON)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(m_log_alphas, model.mse_path_,':')\n",
    "# print(model.mse_path_.shape)\n",
    "plt.plot(m_log_alphas, model.mse_path_.mean(axis=-1),'k',\n",
    "         label='Average across the folds', linewidth=2)\n",
    "plt.axvline(-np.log10(model.alpha_ + EPSILON), linestyle='--', color='k',\n",
    "            label='alpha CV')\n",
    "plt.legend()\n",
    "plt.xlabel('-log(alpha)') \n",
    "plt.ylabel('Mean square error')\n",
    "plt.title('Mean square error on each fold: Lars '\n",
    "          '(train time: %.2fs)' % t_lasso_lars_cv) \n",
    "plt.axis('tight')\n",
    "plt.ylim(ymin, ymax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.18.32 Multiclass sparse logisitic regression on newgroups20\n",
    "\n",
    "Comparison of multinomial logistic L1 vs one-versus-rest L1 logistic regression to classify documents from the newgroups20 dataset. Multinomial logistic regression yields more accurate results and is faster to train on the larger scale dataset.\n",
    "\n",
    "Here we use the l1 sparsity that trims the weights of not informative features to zero. This is good if the goal is to extract the strongly discriminative vocabulary of each class. If the goal is to get the best predictive accuracy, it is better to use the non sparsity-inducing l2 penalty instead.\n",
    "\n",
    "A more traditional (and possibly better) way to predict on a sparse subset of input features would be to use univariate feature selection followed by a traditional (l2-penalised) logistic regression model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups_vectorized\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# warnings.filterwarnings('ignore', category=ConvergenceWarning,\n",
    "#                          module='sklearn')\n",
    "t0 = timeit.default_timer()\n",
    "solver = 'saga'\n",
    "# Turn down for faster run time\n",
    "n_samples = 10000\n",
    "# Memorized fetch_rcvl for faster access\n",
    "dataset = fetch_20newsgroups_vectorized('all')\n",
    "X = dataset.data[:n_samples]\n",
    "y = dataset.target[:n_samples]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y,\n",
    "                                                    test_size=0.1)\n",
    "train_samples, n_features = X_train.shape\n",
    "n_classes = np.unique(y).shape[0]\n",
    "print('Dataset 20newsgroup, train_samples=%i, n_features=%i, n_classes=%i' \n",
    "      % (train_samples, n_features, n_classes))\n",
    "models = {'ovr':{'name':'One versus Rest','iter':[1,2,4]},\n",
    "          'multinomial':{'name':'Multinomial','iter':[1,3,7]}}\n",
    "for model in models:\n",
    "    # Add initial chance-level values for plotting purpose\n",
    "    accuracies = [1/n_classes]\n",
    "    times = [0]\n",
    "    densities = [1]\n",
    "    model_params = models[model]\n",
    "    for this_max_iter in model_params['iter']:\n",
    "        print('[model=%s, solver=%s] Number of epochs: %s' %\n",
    "              (model_params['name'], solver, this_max_iter)) \n",
    "        lr = LogisticRegression(solver=solver,\n",
    "                                multi_class=model,\n",
    "                                C=1,\n",
    "                                penalty='l1',\n",
    "                                fit_intercept=True,\n",
    "                                max_iter=this_max_iter,\n",
    "                                random_state=42)\n",
    "        t1 = timeit.default_timer()\n",
    "        lr.fit(X_train, y_train)\n",
    "        train_time = timeit.default_timer() - t1\n",
    "        y_pred = lr.predict(X_test)\n",
    "        accuracy = np.mean(y_pred==y_test)\n",
    "        density = np.mean(lr.coef_ != 0, axis=1) * 100\n",
    "        accuracies.append(accuracy)\n",
    "        densities.append(density)\n",
    "        times.append(train_time)\n",
    "    models[model]['times'] = times\n",
    "    models[model]['densities'] = densities\n",
    "    models[model]['accuracies'] = accuracies\n",
    "    print('Test accuracy for model %s: %.4f' % (model, accuracies[-1])) \n",
    "    print('%% non-zero coefficients for model %s, '\n",
    "          'per class:\\n %s' % (model, densities[-1]))\n",
    "    print('Run time (%i epochs) for model %s:'\n",
    "          '%.2f' % (model_params['iter'][-1], model, times[-1]))\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "for model in models:\n",
    "    name = models[model]['name']\n",
    "    times = models[model]['times']\n",
    "    accuracies = models[model]['accuracies']\n",
    "    ax.plot(times, accuracies, marker='o',\n",
    "            label='Model: %s' % name) \n",
    "    ax.set_xlabel('Train time (s)') \n",
    "    ax.set_ylabel('Test accuracy')\n",
    "ax.legend()\n",
    "fig.suptitle('Multinomial vs One-vs-Rest Logistic L1\\n' \n",
    "             'Dataset %s' % '20newsgroups') \n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.85)\n",
    "run_time = timeit.default_timer() - t0\n",
    "print('Example run in %.3f s' % run_time) \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
