{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 A tutorial on statistical-learning for scientiﬁc data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Statistical learning: the setting and the estimator object in scikit-learn \n",
    "\n",
    "#### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets \n",
    "iris = datasets.load_iris()\n",
    "data = iris.data\n",
    "data.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimaters objects\n",
    "**Fittingdata**: the main API implemented by scikit-learn is that of the estimator. An estimator is any object that learns from data; it may be a classiﬁcation, regression or clustering algorithm or a transformer that extracts/ﬁlters useful features from raw data. All estimator objects expose a fit method that takes a dataset (usually a 2-d array):\n",
    "> estimator.fit(data)\n",
    "\n",
    "**Estimator parameters**: All the parameters of an estimator can be set when it is instantiated or by modifying the corresponding attribute:\n",
    "> estimator = Estimator(param1=1, param2=2)\n",
    "\n",
    "> estimator.param1\n",
    "\n",
    "**Estimatedparameters**: When data is ﬁtted with an estimator, parameters are estimated from the data at hand. All the estimated parameters are attributes of the estimator object ending by an underscore:\n",
    "> estimator.estimated_param_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Supervised learning: predicting an output variable from high-dimensional observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "iris_X = iris.data\n",
    "iris_y = iris.target\n",
    "np.unique(iris_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-Nearest neighbors classiﬁer\n",
    "\n",
    "The simplest possible classiﬁer is the nearest neighbor: given a new observation X_test, ﬁnd in the training set (i.e. the data used to train the estimator) the observation with the closest feature vector. (Please see the Nearest Neighbors section of the online Scikit-learn documentation for more information about this type of classiﬁer.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "indices = np.random.permutation(len(iris_X))\n",
    "iris_X_train = iris_X[indices[:-10]]\n",
    "iris_y_train = iris_y[indices[:-10]]\n",
    "iris_X_test = iris_X[indices[-10:]]\n",
    "iris_y_test = iris_y[indices[-10:]]\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(iris_X_train,iris_y_train)\n",
    "print(knn.predict(iris_X_test))\n",
    "print(iris_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The curse of dimensionality\n",
    "For an estimator to be effective, you need the distance between neighboring points to be less than some value d,which depends on the problem. In one dimension, this requires on average n ∼ 1/d points. In the context of the above k-NN example, if the data is described by just one feature with values ranging from 0 to 1 and with n training observations, then new data will be no further away than 1/n. Therefore, the nearest neighbor decision rule will be efﬁcient as soon as 1/n is small compared to the scale of between-class feature variations.\n",
    "\n",
    "If the number of features is p, you now require n ∼ 1/$d^p$ points. Let’s say that we require 10 points in one dimension: now $10^p$ points are required in p dimensions to pave the [0,1] space. As p becomes large, the number of training points required for a good estimator grows exponentially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear model: from regression to sparsity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = datasets.load_diabetes()\n",
    "diabetes_X_train = diabetes.data[:-20]\n",
    "diabetes_X_test = diabetes.data[-20:]\n",
    "diabetes_y_train = diabetes.target[:-20]\n",
    "diabetes_y_test = diabetes.target[-20:]\n",
    "from sklearn import linear_model\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(diabetes_X_train,diabetes_y_train)\n",
    "print(regr.coef_)\n",
    "# the mean squre error\n",
    "print(np.mean((regr.predict(diabetes_X_test)-diabetes_y_test)**2))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "# and 0 means that there is no linear relationship\n",
    "# between X and y. \n",
    "regr.score(diabetes_X_test,diabetes_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shrinkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.c_[ .5, 1].T\n",
    "y = [.5, 1]\n",
    "test = np.c_[ 0, 2].T\n",
    "regr = linear_model.LinearRegression()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure()\n",
    "np.random.seed(0)\n",
    "for _ in range(6): \n",
    "    this_X = .1 * np.random.normal(size=(2, 1)) + X\n",
    "    regr.fit(this_X, y)\n",
    "    plt.plot(test, regr.predict(test))\n",
    "    plt.scatter(this_X, y, s=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = linear_model.Ridge(alpha=.1)\n",
    "plt.figure()\n",
    "np.random.seed(0)\n",
    "for _ in range(6): \n",
    "    this_X = .1 * np.random.normal(size=(2, 1)) + X\n",
    "    regr.fit(this_X, y)\n",
    "    plt.plot(test, regr.predict(test))\n",
    "    plt.scatter(this_X, y, s=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.logspace(-4, -1, 6)\n",
    "print([regr.set_params(alpha=alpha)\n",
    "             .fit(diabetes_X_train,diabetes_y_train)\n",
    "             .score(diabetes_X_test,diabetes_y_test)\n",
    "             for alpha in alphas])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of bias/variance tradeoff: the larger the ridge alpha parameter, the higher the bias and the lower the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparsity\n",
    "\n",
    "To improve the conditioning of the problem (i.e. mitigating the The curse of dimensionality), it would be interesting to select only the informative features and set non-informative ones, like feature 2 to 0. Ridge regression will decrease their contribution, but not set them to zero. Another penalization approach, called Lasso (least absolute shrinkage and selection operator), can set some coefﬁcients to zero. Such methods are called sparse method and sparsity can be seen as an application of Occam’s razor: prefer simpler models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = linear_model.Lasso()\n",
    "scores = [regr.set_params(alpha=alpha)\n",
    "             .fit(diabetes_X_train,diabetes_y_train)\n",
    "             .score(diabetes_X_test,diabetes_y_test)\n",
    "             for alpha in alphas]\n",
    "best_alpha = alphas[scores.index(max(scores))]\n",
    "regr.alpha = best_alpha\n",
    "regr.fit(diabetes_X_train,diabetes_y_train)\n",
    "print(regr.coef_) # set contributions of two parameters to zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification\n",
    "\n",
    "For classiﬁcation, as in the labeling iris task, linear regression is not the right approach as it will give too much weight to data far from the decision frontier. A linear approach is to ﬁt a sigmoid function or logistic function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = linear_model.LogisticRegression(solver='lbfgs',C=1e5,\n",
    "                                     multi_class='multinomial')\n",
    "log.fit(iris_X_train,iris_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log.predict(iris_X_test))\n",
    "print(iris_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support vector machines (SVMs)\n",
    "\n",
    "##### Linear SVMs\n",
    "Support Vector Machines belong to the discriminant model family: they try to ﬁnd a combination of samples to build a plane maximizing the margin between the two classes. Regularization is set by the C parameter: a small value for C means the margin is calculated using many or all of the observations around the separating line (more regularization); a large value for C means the margin is calculated on observations close to the separating line (less regularization).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "svc = svm.SVC(kernel='linear')\n",
    "svc.fit(iris_X_train, iris_y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using kernels\n",
    "Classes are not always linearly separable in feature space. The solution is to build a decision function that is not linear but may be polynomial instead. This is done using the kernel trick that can be seen as creating a decision energy by positioning kernels on observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial kernel\n",
    "svc = svm.SVC(kernel='poly',degree=3) \n",
    "# RBFkernel(RadialBasisFunction)\n",
    "svc = svm.SVC(kernel='rbf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Model selection: choosing estimators and their parameters\n",
    "\n",
    "#### Score, and cross-validated scores\n",
    "As we have seen, every estimator exposes a score method that can judge the quality of the ﬁt (or the prediction) on new data. Bigger is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets,svm\n",
    "digits = datasets.load_digits()\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "svc = svm.SVC(C=1, kernel='linear')\n",
    "svc.fit(X_digits[:-100], y_digits[:-100]). \\\n",
    "    score(X_digits[-100:],y_digits[-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_folds = np.array_split(X_digits, 3)\n",
    "y_folds = np.array_split(y_digits, 3)\n",
    "scores = list()\n",
    "for k in range(3):\n",
    "    X_train = list(X_folds) # to make a shallow copy\n",
    "    X_test = X_train.pop()\n",
    "    X_train = np.concatenate(X_train)\n",
    "    y_train = list(y_folds)\n",
    "    y_test = y_train.pop()\n",
    "    y_train = np.concatenate(y_train)\n",
    "    scores.append(svc.fit(X_train, y_train).score(X_test, y_test))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-validation generators\n",
    "split method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "X = [\"a\", \"a\", \"a\", \"b\", \"b\", \"c\", \"c\", \"c\", \"c\", \"c\"]\n",
    "k_fold = KFold(n_splits=5)\n",
    "for train_indices, test_indices in k_fold.split(X):\n",
    "    print('Train: %s | test: %s' % (train_indices, test_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[svc.fit(X_digits[train], y_digits[train]).\n",
    " score(X_digits[test],y_digits[test])\n",
    " for train, test in k_fold.split(X_digits)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "metrics module to achieve that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(svc, X_digits, y_digits, cv=k_fold, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross-validation generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits it into K folds, trains on K-1 and then tests on the left-out\n",
    "from sklearn.model_selection import KFold\n",
    "# Same as K-Fold but preserves the class distribution within each fold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# Ensures that the same group is not in both testing and training sets\n",
    "from sklearn.model_selection import GroupKFold\n",
    "# Generates train/test indices based on random permutation\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "# Same as shufﬂe split but preserves the class distribution within each iteration\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "# Ensures that the same group is not in both testing and training sets\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "# Takes a group array to group observations\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "# Leave P groups out\n",
    "from sklearn.model_selection import LeavePGroupsOut\n",
    "# leave one observation out\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "# Leave P observations out\n",
    "from sklearn.model_selection import LeavePOut\n",
    "# Generates train/test indices based on predeﬁned splits\n",
    "from sklearn.model_selection import PredefinedSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid-search and cross-validated estimators\n",
    "\n",
    "##### Grid-search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "Cs = np.logspace(-6,-1,10)\n",
    "clf = GridSearchCV(estimator=svc, param_grid=dict(C=Cs),\n",
    "                   n_jobs=-1)\n",
    "clf.fit(X_digits[:1000],y_digits[:1000])\n",
    "print(clf.best_score_)\n",
    "print(clf.best_estimator_)\n",
    "print(clf.best_estimator_.C)\n",
    "clf.score(X_digits[1000:],y_digits[1000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nested cross-validation\n",
    "\n",
    "Two cross-validation loops are performed in parallel: one by the GridSearchCV estimator to set gamma andthe other one by cross_val_score to measure the prediction performance of the estimator. The resulting scores are unbiased estimates of the prediction score on new data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(clf, X_digits, y_digits,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-validated estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model, datasets\n",
    "lasso = linear_model.LassoCV(cv=3)\n",
    "diabetes = datasets.load_diabetes()\n",
    "X_diabetes = diabetes.data\n",
    "y_diabetes = diabetes.target\n",
    "lasso.fit(X_diabetes,y_diabetes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The estimator chose automatically its lambda\n",
    "lasso.alpha_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 Unsupervised learning: seeking representations of the data\n",
    "\n",
    "#### Clustering: grouping observations together\n",
    "\n",
    "##### K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster, datasets\n",
    "iris = datasets.load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "\n",
    "k_means = cluster.KMeans(n_clusters=3)\n",
    "k_means.fit(X_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(k_means.labels_[::10])\n",
    "print(y_iris[::10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Applicationexample: vectorquantization\n",
    "Clustering in general and KMeans, in particular, can be seen as a way of choosing a small number of exemplars to compress the information. The problem is sometimes known as vector quantization. For instance, this can be used to posterize an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "try:\n",
    "    face = sp.face(gray=True)\n",
    "except AttributeError:\n",
    "    from scipy import misc\n",
    "    face = misc.face(gray=True)\n",
    "X = face.reshape((-1,1)) # we need an (n_sample,n_feature) array\n",
    "print(X.shape)\n",
    "k_means = cluster.KMeans(n_clusters=5, n_init=1)\n",
    "k_means.fit(X)\n",
    "values = k_means.cluster_centers_.squeeze()\n",
    "print(k_means.cluster_centers_)\n",
    "print(values[:10])\n",
    "labels = k_means.labels_\n",
    "print(labels[:10])\n",
    "face_compressed = np.choose(labels, values)\n",
    "print(face_compressed[:10])\n",
    "face_compressed.shape = face.shape\n",
    "print(face_compressed.shape)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "fig, aes = plt.subplots(2,2,figsize=(10,8))\n",
    "aes[0,0].imshow(face,cmap='gray')\n",
    "aes[0,1].imshow(face_compressed,cmap='gray')\n",
    "aes[1,0].hist(X,bins=50)\n",
    "aes[1,0].set_xlim(0,250)\n",
    "aes[1,1].hist(face_compressed.reshape(-1,1),bins=50)\n",
    "aes[1,1].set_xlim(0,250)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hierarchical agglomerative clustering: Ward\n",
    "\n",
    "- **Agglomerative** - bottom-up approaches: each observation starts in its own cluster, and clusters are iteratively merged in such a way to minimize a linkage criterion. This approach is particularly interesting when the clusters of interest are made of only a few observations. When the number of clusters is large, it is much more computationally efﬁcient than k-means.\n",
    "- **Divisive** - top-down approaches: all observations start in one cluster, which is iteratively split as one moves downthehierarchy. For estimating large numbers of clusters, this approach is both slow(due to all observations starting as one cluster, which it splits recursively) and statistically ill-posed.\n",
    "\n",
    "###### Connectivity-constrained clustering (tbu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage \n",
    "from skimage.data import coins \n",
    "from skimage.transform import rescale\n",
    "from sklearn.feature_extraction.image import grid_to_graph \n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "# these were introduced in skimage-0.14 \n",
    "# if LooseVersion(skimage.__version__) >= '0.14': \n",
    "#     rescale_params = {'anti_aliasing': False, 'multichannel': False} \n",
    "# else: \n",
    "#     rescale_params = {}\n",
    "# ############################################################################# \n",
    "# Generate data \n",
    "orig_coins = coins()\n",
    "# Resize it to 20% of the original size to speed up the processing \n",
    "# Applying a Gaussian filter for smoothing prior to down-scaling \n",
    "# reduces aliasing artifacts. \n",
    "smoothened_coins = gaussian_filter(orig_coins, sigma=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature agglomeration\n",
    "We have seen that sparsity could be used to mitigate the curse of dimensionality, i.e an insufﬁcient amount of observations compared to the number of features. Another approach is to merge together similar features: feature agglomeration. This approach can be implemented by clustering in the feature direction, in other words clustering the transposed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "images = digits.images\n",
    "print(images.shape)\n",
    "X = np.reshape(images,(len(images),-1))\n",
    "print(X.shape)\n",
    "connectivity = grid_to_graph(*images[0].shape)\n",
    "agglo = cluster.FeatureAgglomeration(connectivity=connectivity,\n",
    "                                     n_clusters=32)\n",
    "agglo.fit(X)\n",
    "X_reduced = agglo.transform(X)\n",
    "X_approx = agglo.inverse_transform(X_reduced)\n",
    "images_approx = np.reshape(X_approx, images.shape)\n",
    "print(images_approx.shape)\n",
    "fig, aes = plt.subplots(1,2,figsize=(10,8))\n",
    "aes[0].imshow(images[0],cmap='gray')\n",
    "aes[1].imshow(images_approx[0],cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decompositions: from a signal to components and loadings\n",
    "If X is our multivariate data, then the problem that we are trying to solve is to rewrite it on a different observational basis: we want to learn loadings L and a set of components C such that X = L C. Different criteria exist to choose the components.\n",
    "##### Principal component analysis: PCA\n",
    "Principal component analysis (PCA) selects the successive components that explain the maximum variance in the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.random.normal(size=100)\n",
    "x2 = np.random.normal(size=100)\n",
    "x3 = x1+x2\n",
    "X = np.c_[x1,x2,x3]\n",
    "\n",
    "from sklearn import decomposition\n",
    "pca = decomposition.PCA()\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_)\n",
    "pca.n_components = 2\n",
    "X_reduced = pca.fit_transform(X)\n",
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Independent Component Analysis: ICA\n",
    "Independent component analysis (ICA) selects components so that the distribution of their loadings carries a maximum amount of independent information. It is able to recover non-Gaussian independent signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal\n",
    "time = np.linspace(0,10,2000)\n",
    "s1 = np.sin(2*time) # Signal 1 : sinusoidal signal\n",
    "s2 = np.sign(np.sin(3*time)) # Signal 2 : square signal\n",
    "s3 = signal.sawtooth(2 * np.pi * time) # Signal 3: saw tooth signal \n",
    "S = np.c_[s1,s2,s3]\n",
    "S += 0.2 * np.random.normal(size=S.shape) # Add noise\n",
    "S /= S.std(axis=0) # Standardize data \n",
    "A = np.array([[1, 1, 1], [0.5, 2, 1], [1.5, 1, 2]]) # Mixing matrix \n",
    "X = np.dot(S, A.T) # Generate observations\n",
    "# Compute ICA\n",
    "ica = decomposition.FastICA()\n",
    "S_ = ica.fit_transform(X)\n",
    "A_ = ica.mixing_.T\n",
    "np.allclose(X, np.dot(S_,A_) + ica.mean_)\n",
    "pca = decomposition.PCA()\n",
    "S2_ = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, aes = plt.subplots(4,1,figsize=(15,15))\n",
    "aes[0].plot(S)\n",
    "aes[0].set_title('True sources')\n",
    "aes[1].plot(X)\n",
    "aes[1].set_title('Observations(mixed signal)')\n",
    "aes[2].plot(S_)\n",
    "aes[2].set_title('ICA recovered signal')\n",
    "aes[3].plot(S2_)\n",
    "aes[3].set_title('PCA recovered signal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5 Putting it all together\n",
    "#### Pipelining\n",
    "We have seen that some estimators can transform data and that some estimators can predict variables. We can also create combined estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Define a pipeline to search for the best combination of PCA truncation \n",
    "# and classifier regularization\n",
    "logistic = SGDClassifier(loss='log',\n",
    "                         penalty='l2',\n",
    "                         early_stopping=True,\n",
    "                         max_iter=10000,\n",
    "                         tol=1e-5,\n",
    "                         random_state=0)\n",
    "pca = PCA()\n",
    "pipe = Pipeline(steps=[('pca',pca),('logistic',logistic)])\n",
    "digits = datasets.load_digits() \n",
    "X_digits = digits.data \n",
    "y_digits = digits.target\n",
    "para_grid = {\n",
    "    'pca__n_components':[5,20,30,40,50,64],\n",
    "    'logistic__alpha':np.logspace(-4,4,5)\n",
    "}\n",
    "search = GridSearchCV(pipe, para_grid, iid=False,cv=5)\n",
    "search.fit(X_digits, y_digits)\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_) \n",
    "print(search.best_params_)\n",
    "\n",
    "# plot the pca spectrum\n",
    "pca.fit(X_digits)\n",
    "fig, (ax0, ax1) = plt.subplots(nrows=2, sharex=True, figsize=(6,6))\n",
    "ax0.plot(pca.explained_variance_ratio_, linewidth=2)\n",
    "ax0.set_ylabel('PCA explained variance')\n",
    "ax0.axvline(search.best_estimator_.named_steps['pca'].n_components, \n",
    "            linestyle=':', label='n_components chosen')\n",
    "\n",
    "ax1.plot([5,20,30,40,50,64],search.cv_results_['mean_test_score'].reshape(5,6).T.max(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Face recognition with eigenfaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time \n",
    "import logging \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.datasets import fetch_lfw_people \n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.svm import SVC\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "n_samples, h, w = lfw_people.images.shape\n",
    "X = lfw_people.data \n",
    "n_features = X.shape[1]\n",
    "y = lfw_people.target\n",
    "target_names = lfw_people.target_names \n",
    "# print(target_names)\n",
    "n_classes = target_names.shape[0]\n",
    "print(\"Total dataset size:\") \n",
    "print(\"n_samples: %d\" % n_samples) \n",
    "print(\"n_features: %d\" % n_features) \n",
    "print(\"n_classes: %d\" % n_classes)\n",
    "\n",
    "# Split into a training set and a test set using a stratified k fold\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.25, \n",
    "                                                    random_state=42)\n",
    "\n",
    "# Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled \n",
    "# dataset): unsupervised feature extraction / dimensionality reduction \n",
    "n_components = 150\n",
    "print(\"Extracting the top %d eigenfaces from %d faces\" \n",
    "      % (n_components, X_train.shape[0])) \n",
    "t0 = time() \n",
    "pca = PCA(n_components=n_components, \n",
    "          svd_solver='randomized', \n",
    "          whiten=True).fit(X_train) \n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "eigenfaces = pca.components_.reshape((n_components, h, w))\n",
    "print(\"Projecting the input data on the eigenfaces orthonormal basis\") \n",
    "t0 = time()\n",
    "X_train_pca = pca.transform(X_train) \n",
    "X_test_pca = pca.transform(X_test) \n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "# Train a SVM classification model\n",
    "print(\"Fitting the classifier to the training set\")\n",
    "t0 = time()\n",
    "param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5], \n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], } \n",
    "clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'),\n",
    "                   param_grid, cv=5, iid=False)  \n",
    "clf = clf.fit(X_train_pca, y_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0)) \n",
    "print(\"Best estimator found by grid search:\") \n",
    "print(clf.best_estimator_)\n",
    "\n",
    "# Quantitative evaluation of the model quality on the test set\n",
    "print(\"Predicting people's names on the test set\") \n",
    "t0 = time() \n",
    "y_pred = clf.predict(X_test_pca) \n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print(classification_report(y_test, y_pred, target_names=target_names)) \n",
    "print(confusion_matrix(y_test, y_pred, labels=range(n_classes)))\n",
    "\n",
    "# Qualitative evaluation of the predictions using matplotlib\n",
    "def plot_gallery(images, titles, h, w, n_row=3, n_col=4): \n",
    "    \"\"\"Helper function to plot a gallery of portraits\"\"\" \n",
    "    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row)) \n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(n_row * n_col): \n",
    "        plt.subplot(n_row, n_col, i + 1) \n",
    "        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray) \n",
    "        plt.title(titles[i], size=12) \n",
    "        plt.xticks(()) \n",
    "        plt.yticks(())\n",
    "# plot the result of the prediction on a portion of the test set\n",
    "def title(y_pred, y_test, target_names, i): \n",
    "    pred_name = target_names[y_pred[i]].rsplit(' ', 1)[-1] \n",
    "    true_name = target_names[y_test[i]].rsplit(' ', 1)[-1] \n",
    "    return 'predicted: %s\\ntrue: %s' % (pred_name, true_name)\n",
    "prediction_titles = [title(y_pred, y_test, target_names, i) \n",
    "                     for i in range(y_pred.shape[0])]\n",
    "plot_gallery(X_test, prediction_titles, h, w)\n",
    "eigenface_titles = [\"eigenface %d\" % i \n",
    "                    for i in range(eigenfaces.shape[0])] \n",
    "plot_gallery(eigenfaces, eigenface_titles, h, w)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
