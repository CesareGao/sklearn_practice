{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Working With Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this guide is to explore some of the main scikit-learn tools on a single practical task: analyzing a collection of text documents (newsgroups posts) on twenty different topics. \n",
    "\n",
    "In this section we will see how to: \n",
    "- load the ﬁle contents and the categories\n",
    "- extract feature vectors suitable for machine learning \n",
    "- train a linear model to perform categorization \n",
    "- use a grid search strategy to ﬁnd a good conﬁguration of both the feature extraction components and the classiﬁer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Loading the 20 newsgroups dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian', \n",
    "               'comp.graphics', 'sci.med']\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "                                  categories=categories,\n",
    "                                  shuffle=True,\n",
    "                                  random_state=42)\n",
    "print(twenty_train.target_names)\n",
    "print(len(twenty_train.data))\n",
    "print(len(twenty_train.filenames))\n",
    "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:3])) \n",
    "print(twenty_train.target_names[twenty_train.target[0]]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Extracting features from text ﬁles\n",
    "In order to perform machine learning on text documents, we ﬁrst need to turn the text content into numerical feature vectors.\n",
    "#### Bags of words\n",
    "The most intuitive way to do so is to use a bags of words representation:\n",
    "1. Assign a ﬁxed integer id to each word occurring in any document of the training set (for instance by building a dictionary from words to integer indices). \n",
    "1. For each document #i, count the number of occurrences of each word w and store it in X[i, j] as the value of feature #j where j is the index of word w in the dictionary. \n",
    "\n",
    "Fortunately, most values in X will be zeros since for a given document less than a few thousand distinct words will beused. For this reason we say that bags of words are typically high-dimensional sparse datasets. We can save a lot of memory by only storing the non-zero parts of the feature vectors in memory\n",
    "\n",
    "##### Tokenizing text with scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "print(count_vect.vocabulary_.get(u'algorithm'))\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From occurrences to frequencies\n",
    "\n",
    "Occurrence count is a good start but there is an issue: longer documents will have higher average count values than shorter documents, even though they might talk about the same topics. \n",
    "\n",
    "To avoid these potential discrepancies it sufﬁces to divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called tf for Term Frequencies. \n",
    "\n",
    "Another reﬁnement on top of tf is to down scale weights forwords that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus. \n",
    "\n",
    "This downscaling is called tf–idf for “Term Frequency times Inverse Document Frequency”. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "print(X_train_tf[0,177])\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer().fit(X_train_counts)\n",
    "X_train_tfidf = tfidf_transformer.transform(X_train_counts)\n",
    "print(X_train_tf[0,177])\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4 Training a classiﬁer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf,twenty_train.target)\n",
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "for doc, category in zip(docs_new,predicted):\n",
    "    print('%r => %s' % (doc, twenty_train.target_names[category])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.5 Building a pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([\n",
    "    ('vect',CountVectorizer()),\n",
    "    ('tfidf',TfidfTransformer()),\n",
    "    ('clf',MultinomialNB())\n",
    "])\n",
    "text_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.6 Evaluation of the performance on the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "                                 categories=categories,\n",
    "                                 shuffle=True,\n",
    "                                 random_state=42)\n",
    "docs_test = twenty_test.data\n",
    "predicted = text_clf.predict(docs_test)\n",
    "np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let’s see if we can do better with a linear support vector machine (SVM), which is widely regarded as one of the best text classiﬁcation algorithms (although it’s also a bit slower than naïve Bayes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf = Pipeline([\n",
    "    ('vect',CountVectorizer()),\n",
    "    ('tfidf',TfidfTransformer()),\n",
    "    ('clf',SGDClassifier(loss='hinge',\n",
    "                         penalty='l2',\n",
    "                         alpha=1e-3,\n",
    "                         random_state=42,\n",
    "                         max_iter=5,\n",
    "                         tol=None))\n",
    "])\n",
    "text_clf.fit(twenty_train.data,twenty_train.target)\n",
    "predicted = text_clf.predict(docs_test)\n",
    "np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(twenty_test.target,predicted,\n",
    "                            target_names=twenty_test.target_names))\n",
    "print(confusion_matrix(twenty_test.target,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.7 Parameter tuning using grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "    'vect__ngram_range': [(1,1),(1,2)],\n",
    "    'tfidf__use_idf': (True,False),\n",
    "    'clf__alpha': (1e-2,1e-3)\n",
    "}\n",
    "gs_clf = GridSearchCV(text_clf,parameters,cv=5,iid=False,n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(twenty_train.data[:400], twenty_train.target[:400])\n",
    "print(twenty_train.target_names[gs_clf.predict(['God is love'])[0]])\n",
    "print(gs_clf.best_score_)\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %s\" % (param_name,gs_clf.best_params_[param_name]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
